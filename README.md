# LawLLM - RAG pour jugements suisses

Un systÃ¨me de RAG (Retrieval-Augmented Generation) pour rechercher et analyser les jugements judiciaires suisses depuis [entscheidsuche.ch](https://entscheidsuche.ch).

## ğŸ¯ FonctionnalitÃ©s

- **Scraping modulaire** : TÃ©lÃ©chargement efficace des jugements depuis l'API Entscheidsuche
- **Extraction de texte** : Support HTML et PDF avec extraction automatique
- **Architecture modulaire** : Code organisÃ© et facilement extensible
- **Tests intÃ©grÃ©s** : Structure de tests pour garantir la qualitÃ©
- **PrÃªt pour RAG** : Structure prÃªte pour l'ajout de couches d'embeddings et de recherche vectorielle

## ğŸ“ Structure du projet

```
lawllm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py          # Point d'entrÃ©e principal
â”‚   â”œâ”€â”€ models.py            # ModÃ¨les de donnÃ©es (Pydantic)
â”‚   â”œâ”€â”€ scraper/             # Module de scraping
â”‚   â”‚   â”œâ”€â”€ __init__.py      # API principale du scraper
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration
â”‚   â”‚   â”œâ”€â”€ client.py        # Client HTTP avec rate limiting
â”‚   â”‚   â”œâ”€â”€ extractors.py    # Extraction de texte (HTML/PDF)
â”‚   â”‚   â”œâ”€â”€ storage.py        # Sauvegarde des documents
â”‚   â”‚   â”œâ”€â”€ normalizer.py    # Normalisation des mÃ©tadonnÃ©es
â”‚   â”‚   â””â”€â”€ spider_registry.py # Gestion des spiders
â”‚   â””â”€â”€ tests/               # Tests unitaires
â”‚       â”œâ”€â”€ test_models.py
â”‚       â”œâ”€â”€ test_scraper.py
â”‚       â””â”€â”€ conftest.py
â”œâ”€â”€ data/                    # DonnÃ©es scrapÃ©es (crÃ©Ã© automatiquement)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â””â”€â”€ README.md
```

## ğŸš€ Installation

### Installation standard

```bash
# Cloner le repository
git clone <url>
cd lawllm

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Installation en mode dÃ©veloppement (recommandÃ©)

Pour que les imports fonctionnent correctement depuis n'importe oÃ¹ :

```bash
# Installer le package en mode dÃ©veloppement
pip install -e .

# Ou avec les dÃ©pendances de dÃ©veloppement
pip install -e ".[dev]"
```

Cela permet d'importer `src` depuis n'importe quel script du projet.

## ğŸ’» Utilisation

### Scraper un spider

```python
import asyncio
from src import scrape_spider

# Scraper 100 documents du Tribunal fÃ©dÃ©ral en franÃ§ais
async def main():
    documents = await scrape_spider(
        spider="CH_BGer",
        max_docs=100,
        language="fr"
    )
    print(f"ScrapÃ© {len(documents)} documents")

asyncio.run(main())
```

### Utiliser le scraper avec configuration personnalisÃ©e

```python
import asyncio
from src import EntscheidsucheScraper, ScraperConfig

async def main():
    # Configuration personnalisÃ©e
    config = ScraperConfig(
        output_dir="data/custom",
        rate_limit=10,  # 10 requÃªtes/seconde
        max_concurrent=20
    )
    
    async with EntscheidsucheScraper(config) as scraper:
        async for doc in scraper.fetch_spider("CH_BGer", max_docs=50):
            print(f"Document: {doc.id}")
            print(f"Titre: {doc.title}")
            print(f"Contenu: {len(doc.content)} caractÃ¨res")
            # Le document est automatiquement sauvegardÃ©

asyncio.run(main())
```

### Lister les spiders disponibles

```python
from src import list_available_spiders

spiders = list_available_spiders()
for spider_id, description in spiders.items():
    print(f"{spider_id}: {description}")
```

### Scraper plusieurs spiders

```python
import asyncio
from src import EntscheidsucheScraper, ScraperConfig

async def main():
    config = ScraperConfig(output_dir="data/raw")
    
    async with EntscheidsucheScraper(config) as scraper:
        spiders = ["CH_BGer", "CH_BGE"]
        async for doc in scraper.fetch_spiders(
            spiders=spiders,
            max_docs_per_spider=10,
            language="fr"
        ):
            print(f"{doc.spider}: {doc.id}")

asyncio.run(main())
```

### Pipeline complÃ¨te d'indexation dans ChromaDB

Le script `main.py` fournit une interface en ligne de commande complÃ¨te pour ajouter des documents dans ChromaDB. **Par dÃ©faut, il fait du scraping depuis l'API Entscheidsuche**, puis chunking, rÃ©sumÃ© et indexation.

#### Mode par dÃ©faut : Scraping complet

Sans aucune option, le script scrape tous les spiders disponibles :

```bash
python main.py
```

#### Scraper des spiders spÃ©cifiques

Pour scraper uniquement certains spiders (rÃ©gions/tribunaux) :

```bash
python main.py --spiders CH_BGer VD_FindInfo
```

#### Scraper uniquement les nouveaux documents

Pour ne tÃ©lÃ©charger que les documents qui n'existent pas dÃ©jÃ  :

```bash
python main.py --only-new
```

#### Limiter le nombre de documents

Pour limiter le nombre de documents par spider :

```bash
python main.py --spiders CH_BGer --max-docs 50
```

#### Filtrer par langue

Pour ne scraper que les documents dans une langue spÃ©cifique :

```bash
python main.py --language fr
```

#### Indexer depuis des rÃ©sumÃ©s existants (sans scraping)

Si vous avez dÃ©jÃ  des rÃ©sumÃ©s dans `data/summaries` :

```bash
python main.py --from-summaries
```

#### Indexer depuis des chunks (avec rÃ©sumÃ© automatique)

Si vous avez des chunks dans `data/chunks` et que vous voulez les rÃ©sumer puis les indexer :

```bash
python main.py --from-chunks
```

#### Pipeline depuis les documents dÃ©jÃ  scrapÃ©s

Pour faire la pipeline sur des documents dÃ©jÃ  tÃ©lÃ©chargÃ©s (sans scraping) :

```bash
python main.py --from-documents
```

#### Options avancÃ©es

```bash
# RÃ©initialiser la collection avant l'indexation
python main.py --reset

# Utiliser un modÃ¨le Ollama spÃ©cifique pour le rÃ©sumÃ©
python main.py --ollama-model llama3

# SpÃ©cifier un chemin de base ChromaDB personnalisÃ©
python main.py --db-path custom_chroma_db

# Ne pas sauvegarder les fichiers intermÃ©diaires
python main.py --no-save-intermediate

# Utiliser le texte original tronquÃ© au lieu de rÃ©sumer (plus rapide)
python main.py --skip-summarization

# Ajuster la limite de requÃªtes par seconde
python main.py --rate-limit 10

# Afficher l'aide complÃ¨te
python main.py --help
```

#### Exemples de workflow complets

**Workflow 1 : Pipeline complÃ¨te avec scraping (par dÃ©faut)**
```bash
# Scrape tous les spiders, puis chunking, rÃ©sumÃ© et indexation
python main.py
```

**Workflow 2 : Scraper uniquement certains spiders**
```bash
# Scraper uniquement le Tribunal fÃ©dÃ©ral et le canton de Vaud
python main.py --spiders CH_BGer VD_FindInfo --only-new
```

**Workflow 3 : Utiliser des rÃ©sumÃ©s existants (sans scraping)**
```bash
# Si vous avez dÃ©jÃ  des rÃ©sumÃ©s dans data/summaries
python main.py --from-summaries --reset
```

**Workflow 4 : Sans rÃ©sumÃ© (plus rapide)**
```bash
# Utiliser le texte original tronquÃ©, pas de rÃ©sumÃ©
python main.py --skip-summarization
```

**Workflow 5 : Scraping incrÃ©mental**
```bash
# Ne scraper que les nouveaux documents depuis la derniÃ¨re exÃ©cution
python main.py --only-new --spiders CH_BGer
```

**Workflow 6 : AccÃ©lÃ©rer le tÃ©lÃ©chargement**
```bash
# Augmenter le rate limit et le parallÃ©lisme pour tÃ©lÃ©charger plus vite
python main.py --rate-limit 10 --max-concurrent 20

# Configuration agressive (attention aux limites du serveur)
python main.py --rate-limit 20 --max-concurrent 50
```

#### Optimisation des performances

Pour accÃ©lÃ©rer le tÃ©lÃ©chargement de grandes quantitÃ©s de donnÃ©es :

1. **Augmenter le rate limit** : `--rate-limit 10-20` (au lieu de 5 par dÃ©faut)
   - Permet plus de requÃªtes par seconde
   - Attention : respectez les limites du serveur pour Ã©viter les blocages

2. **Augmenter le parallÃ©lisme** : `--max-concurrent 20-50` (au lieu de 10 par dÃ©faut)
   - Permet plus de requÃªtes simultanÃ©es
   - AmÃ©liore l'utilisation de la bande passante

3. **Combiner les deux** :
   ```bash
   python main.py --rate-limit 15 --max-concurrent 30
   ```

**Note** : Le systÃ¨me utilise un rate limiting optimisÃ© avec fenÃªtre glissante qui permet de mieux utiliser le parallÃ©lisme tout en respectant les limites.

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest

# Lancer avec couverture
pytest --cov=src

# Lancer un fichier de test spÃ©cifique
pytest src/tests/test_models.py
```

## ğŸ“¦ Architecture modulaire

Le projet est organisÃ© en modules sÃ©parÃ©s pour faciliter la maintenance et les tests :

- **`config.py`** : Configuration centralisÃ©e avec validation
- **`client.py`** : Client HTTP rÃ©utilisable avec rate limiting
- **`extractors.py`** : Extraction de texte depuis diffÃ©rents formats
- **`storage.py`** : Gestion du stockage des documents
- **`normalizer.py`** : Normalisation des donnÃ©es de l'API
- **`spider_registry.py`** : Gestion du registre des spiders

Cette architecture facilite :
- L'ajout de nouvelles fonctionnalitÃ©s (embeddings, RAG, etc.)
- Les tests unitaires de chaque composant
- La maintenance et le dÃ©bogage

## ğŸ”® Prochaines Ã©tapes

- [ ] Couche d'embeddings avec sentence-transformers
- [ ] Base de donnÃ©es vectorielle (ChromaDB)
- [ ] Interface de recherche RAG
- [ ] API REST pour interroger les documents
- [ ] Interface web

## ğŸ“ Licence

Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.


python .\main.py --spiders VD_FindInfo --from-documents  --embedding-batch-size 50 --skip-summarization --max-workers 32 --device cuda   